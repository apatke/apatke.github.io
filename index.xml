<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Archit Patke</title><link>https://apatke.github.io/</link><description>Recent content in Home on Archit Patke</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor> ()</managingEditor><webMaster> ()</webMaster><lastBuildDate>Wed, 19 Jul 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://apatke.github.io/" rel="self" type="application/rss+xml"/><item><title>QLM: Queue Management for SLO-oriented Large Language Model Serving</title><link>https://apatke.github.io/publications/socc24/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/socc24/</guid><description>&lt;p&gt;Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving. QLM maintains batch and interactive requests across different models and SLOs in a request queue. Optimal ordering of the request queue is critical to maintain SLOs while ensuring high resource utilization. To generate this optimal ordering, QLM uses a Request Waiting Time (RWT) Estimator that estimates the waiting times for requests in the request queue. These estimates are used by a global scheduler to orchestrate LLM Serving Operations (LSOs) such as request pulling, request eviction, load balancing, and model swapping. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems. QLM&amp;rsquo;s evaluation is based on the production requirements of a cloud provider. QLM is publicly available at &lt;a href=&#34;https://www.github.com/QLM-project/QLM&#34;&gt;https://www.github.com/QLM-project/QLM&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Power-aware Deep Learning Model Serving with μ-Serve</title><link>https://apatke.github.io/publications/atc24/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/atc24/</guid><description>&lt;p&gt;With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling. We explore the co-design space and present a novel power-aware model-serving system, µ-Serve. µ-Serve is a model-serving framework that optimizes the power consumption and model serving latency/throughput of serving multiple ML models efficiently in a homogeneous GPU cluster. Evaluation results on production workloads show that µ-Serve achieves 1.2–2.6× power saving by dynamic GPU frequency scaling (up to 61% reduction) without SLO attainment violations.&lt;/p&gt;</description></item><item><title>Delay Sensitivity-driven Congestion Mitigation for HPC Systems</title><link>https://apatke.github.io/publications/ics2021/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/ics2021/</guid><description>&lt;p&gt;Modern high-performance computing (HPC) systems concurrently execute multiple distributed applications that contend for the high-speed network leading to congestion. Consequently, application runtime variability and suboptimal system utilization are observedin production systems. To address these problems, we propose Netscope, a congestion mitigation framework based on a novel delay sensitivity metric that quantifies the impact of congestionon application runtime. Netscope uses delay sensitivity estimates to drive a congestion mitigation mechanism to selectively throttle applications that are less susceptible to congestion. We evaluate Netscope on two Cray Aries systems, including a production supercomputer, on common scientific applications. Our evaluation showsthat Netscope has a low training cost and accurately estimates the impact of congestion on application runtime with a correlation between 0.7 and 0.9. Moreover, Netscope reduces application tail runtime increase by up to 16.3× while improving the median systemutility by 12%.&lt;/p&gt;</description></item><item><title>Measuring Congestion in High-Performance Datacenter Interconnects</title><link>https://apatke.github.io/publications/nsdi2020/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/nsdi2020/</guid><description>&lt;p&gt;While it is widely acknowledged that network congestion
in High Performance Computing (HPC) systems can significantly degrade application performance, there has been little
to no quantification of congestion on credit-based interconnect networks. We present a methodology for detecting, extracting, and characterizing regions of congestion in networks.
We have implemented the methodology in a deployable tool,
Monet, which can provide such analysis and feedback at runtime. Using Monet, we characterize and diagnose congestion
in the world’s largest 3D torus network of Blue Waters, a 13.3-
petaflop supercomputer at the National Center for Supercomputing Applications. Our study deepens the understanding of
production congestion at a scale that has never been evaluated
before.&lt;/p&gt;</description></item></channel></rss>
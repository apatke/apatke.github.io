<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Archit Patke</title><link>https://apatke.github.io/</link><description>Recent content in Home on Archit Patke</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor> ()</managingEditor><webMaster> ()</webMaster><lastBuildDate>Wed, 19 Jul 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://apatke.github.io/" rel="self" type="application/rss+xml"/><item><title>Characterizing GPU resilience in modern AI systems</title><link>https://apatke.github.io/publications/sc25/</link><pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/sc25/</guid><description>&lt;p&gt;In this study, we characterize GPU failures in Delta, the current large-scale AI system with over 600 petaflops of peak compute throughput. The system comprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA A40, A100, and H100 GPUs. The study uses two and a half years of data on GPU errors. We evaluate the resilience of GPU hardware components to determine the vulnerability of different GPU components to failure and their impact on the GPU and node availability. We measure the key propagation paths in GPU hardware, GPU interconnect (NVLink), and GPU memory. Finally, we evaluate the impact of the observed GPU errors on user jobs. Our key findings are: (i) Contrary to common beliefs, GPU memory is over 30x more reliable than GPU hardware in terms of MTBE (mean time between errors). (ii) The newly introduced GSP (GPU System Processor) is the most vulnerable GPU hardware component. (iii) NVLink errors did not always lead to user job failure, and we attribute it to the underlying error detection and retry mechanisms employed. (iv) We show multiple examples of hardware errors originating from one of the key GPU hardware components, leading to application failure. (v) We project the impact of GPU node availability on larger scales with emulation and find that significant overprovisioning between 5-20% would be necessary to handle GPU failures. If GPU availability were improved to 99.9%, the overprovisioning would be reduced by 4x.&lt;/p&gt;</description></item><item><title>Learned Page Migration in Disaggregated HPC Systems</title><link>https://apatke.github.io/publications/ics2025/</link><pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/ics2025/</guid><description>&lt;p&gt;Hardware memory disaggregation (HMD) is an emerging technology that enables access to remote memory, thereby creating expansive memory pools and reducing memory underutilization in datacenters. However, a significant challenge arises when accessing remote memory over a network: increased contention that can lead to severe application performance degradation. To reduce the performance penalty of using remote memory, the operating system uses page migration to promote frequently accessed pages closer to the processor. However, previously proposed page migration mechanisms do not achieve the best performance in HMD systems because of obliviousness to variable page transfer costs that occur due to network contention. To address these limitations, we present INDIGO: a network-aware page migration framework that uses novel page telemetry and a learning-based approach for network adaptation. We implemented INDIGO in the Linux kernel and evaluated it with common cloud and HPC applications on a real disaggregated memory system prototype. Our evaluation shows that INDIGO offers up to 50–70% improvement in application performance compared to other state-of-the-art page migration policies and reduces network traffic up to 2×.&lt;/p&gt;</description></item><item><title>QLM: Queue Management for SLO-oriented Large Language Model Serving</title><link>https://apatke.github.io/publications/socc24/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/socc24/</guid><description>&lt;p&gt;Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving. QLM maintains batch and interactive requests across different models and SLOs in a request queue. Optimal ordering of the request queue is critical to maintain SLOs while ensuring high resource utilization. To generate this optimal ordering, QLM uses a Request Waiting Time (RWT) Estimator that estimates the waiting times for requests in the request queue. These estimates are used by a global scheduler to orchestrate LLM Serving Operations (LSOs) such as request pulling, request eviction, load balancing, and model swapping. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems. QLM&amp;rsquo;s evaluation is based on the production requirements of a cloud provider. QLM is publicly available at &lt;a href=&#34;https://www.github.com/QLM-project/QLM&#34;&gt;https://www.github.com/QLM-project/QLM&lt;/a&gt;.&lt;/p&gt;</description></item><item><title>Power-aware Deep Learning Model Serving with μ-Serve</title><link>https://apatke.github.io/publications/atc24/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/atc24/</guid><description>&lt;p&gt;With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling. We explore the co-design space and present a novel power-aware model-serving system, µ-Serve. µ-Serve is a model-serving framework that optimizes the power consumption and model serving latency/throughput of serving multiple ML models efficiently in a homogeneous GPU cluster. Evaluation results on production workloads show that µ-Serve achieves 1.2–2.6× power saving by dynamic GPU frequency scaling (up to 61% reduction) without SLO attainment violations.&lt;/p&gt;</description></item><item><title>Delay Sensitivity-driven Congestion Mitigation for HPC Systems</title><link>https://apatke.github.io/publications/ics2021/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/ics2021/</guid><description>&lt;p&gt;Modern high-performance computing (HPC) systems concurrently execute multiple distributed applications that contend for the high-speed network leading to congestion. Consequently, application runtime variability and suboptimal system utilization are observedin production systems. To address these problems, we propose Netscope, a congestion mitigation framework based on a novel delay sensitivity metric that quantifies the impact of congestionon application runtime. Netscope uses delay sensitivity estimates to drive a congestion mitigation mechanism to selectively throttle applications that are less susceptible to congestion. We evaluate Netscope on two Cray Aries systems, including a production supercomputer, on common scientific applications. Our evaluation showsthat Netscope has a low training cost and accurately estimates the impact of congestion on application runtime with a correlation between 0.7 and 0.9. Moreover, Netscope reduces application tail runtime increase by up to 16.3× while improving the median systemutility by 12%.&lt;/p&gt;</description></item><item><title>Measuring Congestion in High-Performance Datacenter Interconnects</title><link>https://apatke.github.io/publications/nsdi2020/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><author> ()</author><guid>https://apatke.github.io/publications/nsdi2020/</guid><description>&lt;p&gt;While it is widely acknowledged that network congestion
in High Performance Computing (HPC) systems can significantly degrade application performance, there has been little
to no quantification of congestion on credit-based interconnect networks. We present a methodology for detecting, extracting, and characterizing regions of congestion in networks.
We have implemented the methodology in a deployable tool,
Monet, which can provide such analysis and feedback at runtime. Using Monet, we characterize and diagnose congestion
in the world’s largest 3D torus network of Blue Waters, a 13.3-
petaflop supercomputer at the National Center for Supercomputing Applications. Our study deepens the understanding of
production congestion at a scale that has never been evaluated
before.&lt;/p&gt;</description></item></channel></rss>
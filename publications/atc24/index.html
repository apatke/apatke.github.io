<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.59.1"><meta name=author content="Archit Patke"><meta name=description content="Power-aware Deep Learning Model Serving with μ-Serve"><meta property="og:title" content="Power-aware Deep Learning Model Serving with μ-Serve"><meta property="og:description" content="With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling."><meta property="og:type" content="article"><meta property="og:url" content="https://apatke.github.io/publications/atc24/"><meta property="article:published_time" content="2024-06-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-15T00:00:00+00:00"><meta itemprop=name content="Power-aware Deep Learning Model Serving with μ-Serve"><meta itemprop=description content="With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling."><meta itemprop=datePublished content="2024-06-15T00:00:00&#43;00:00"><meta itemprop=dateModified content="2024-06-15T00:00:00&#43;00:00"><meta itemprop=wordCount content="161"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Power-aware Deep Learning Model Serving with μ-Serve"><meta name=twitter:description content="With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling."><title>Power-Aware Deep Learning Model Serving With Μ-Serve | Archit Patke</title><link rel=canonical href=https://apatke.github.io/publications/atc24/><link rel=icon href=https://apatke.github.io/img/favicon.ico><link rel="shortcut icon" href=https://apatke.github.io/img/favicon.ico><link rel=apple-touch-icon href=https://apatke.github.io/img/favicon.ico><link rel=stylesheet href=/main.min.css><link rel=preload href=https://use.fontawesome.com/releases/v5.3.1/css/all.css crossorigin=anonymous as=style onload="this.rel='stylesheet'"><link rel=preload href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css type=text/css as=style onload="this.rel='stylesheet'"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-167344484-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=padding id=left></div><div id=container><div id=sidebar><div id=enclose-sidebar><div id=sidebar-content><h1><a href=https://apatke.github.io/>Archit Patke</a></h1><p>PhD Candidate<br>Electrical and Computer Engineering<br>University of Illinois at Urbana-Champaign</p><ul class=icon-list><li><a href=mailto:apatke@illinois.edu><i class="fas fa-envelope-open fa-fw"></i></a></li><li><a href="https://scholar.google.com/citations?user=NK4dg2oAAAAJ&amp;hl=en"><i class="ai ai-google-scholar-square ai-fw"></i></a></li><li><a href=https://www.linkedin.com/in/archit-patke-169a46103/><i class="fab fa-linkedin fa-fw"></i></a></li><li><a href=https://github.com/apatke><i class="fab fa-github fa-fw"></i></a></li></ul></div><div id=avatar><img src=https://apatke.github.io/img/me.jpg alt=Avatar></div></div></div><div id=content><ol class=breadcrumbs><li><a href=/><i class="fas fa-home"></i></a></li><li><a href=https://apatke.github.io/publications/>Publications</a></li><li>Power-aware Deep Learning Model Serving with μ-Serve</li></ol><div class=titlesec><h1 class=no-border>Power-aware Deep Learning Model Serving with μ-Serve</h1><h3>Haoran Qiu, Weichao Mao, **Archit Patke**, Shengkun Cui, Saurabh Jha, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Tamer Başar, Ravishankar Iyer,</h3><h3 style=color:#555><b><i>**ATC 2024**</i></b></h3></div><hr><ul class=single-paper-links><li><a href=https://www.usenix.org/conference/atc24/presentation/qiu target=_blank><i class="ai ai-doi" aria-hidden=true></i>&nbsp;DOI</a></li><li><a href=https://apatke.github.io/publications/atc24/Paper.pdf target=_blank><i class="far fa-file-pdf" aria-hidden=true></i>&nbsp;Paper</a></li></ul><h3>Abstract</h3><p>With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling. We explore the co-design space and present a novel power-aware model-serving system, µ-Serve. µ-Serve is a model-serving framework that optimizes the power consumption and model serving latency/throughput of serving multiple ML models efficiently in a homogeneous GPU cluster. Evaluation results on production workloads show that µ-Serve achieves 1.2–2.6× power saving by dynamic GPU frequency scaling (up to 61% reduction) without SLO attainment violations.</p></div></div><div class=padding id=right></div><script src=https://instant.page/3.0.0 type=module defer integrity=sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1 crossorigin=anonymous></script></body></html>
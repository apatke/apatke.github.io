<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Publications on Archit Patke</title><link>https://apatke.github.io/publications/</link><description>Recent content in Publications on Archit Patke</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 20 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://apatke.github.io/publications/index.xml" rel="self" type="application/rss+xml"/><item><title>Characterizing GPU resilience in modern AI systems</title><link>https://apatke.github.io/publications/sc25/</link><pubDate>Sun, 20 Jul 2025 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/sc25/</guid><description>In this study, we characterize GPU failures in Delta, the current large-scale AI system with over 600 petaflops of peak compute throughput. The system comprises GPU and non-GPU nodes with modern AI accelerators, such as NVIDIA A40, A100, and H100 GPUs. The study uses two and a half years of data on GPU errors. We evaluate the resilience of GPU hardware components to determine the vulnerability of different GPU components to failure and their impact on the GPU and node availability.</description></item><item><title>Learned Page Migration in Disaggregated HPC Systems</title><link>https://apatke.github.io/publications/ics2025/</link><pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/ics2025/</guid><description>Hardware memory disaggregation (HMD) is an emerging technology that enables access to remote memory, thereby creating expansive memory pools and reducing memory underutilization in datacenters. However, a significant challenge arises when accessing remote memory over a network: increased contention that can lead to severe application performance degradation. To reduce the performance penalty of using remote memory, the operating system uses page migration to promote frequently accessed pages closer to the processor.</description></item><item><title>QLM: Queue Management for SLO-oriented Large Language Model Serving</title><link>https://apatke.github.io/publications/socc24/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/socc24/</guid><description>Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving.</description></item><item><title>Power-aware Deep Learning Model Serving with μ-Serve</title><link>https://apatke.github.io/publications/atc24/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/atc24/</guid><description>With the increasing popularity of large deep learning model-serving workloads, there is a pressing need to reduce the energy consumption of a model-serving cluster while maintaining satisfied throughput or model-serving latency requirements. Model multiplexing approaches such as model parallelism, model placement, replication, and batching aim to optimize the model-serving performance. However, they fall short of leveraging the GPU frequency scaling opportunity for power saving. In this paper, we demonstrate (1) the benefits of GPU frequency scaling in power saving for model serving; and (2) the necessity for co-design and optimization of fine-grained model multiplexing and GPU frequency scaling.</description></item><item><title>Delay Sensitivity-driven Congestion Mitigation for HPC Systems</title><link>https://apatke.github.io/publications/ics2021/</link><pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/ics2021/</guid><description>Modern high-performance computing (HPC) systems concurrently execute multiple distributed applications that contend for the high-speed network leading to congestion. Consequently, application runtime variability and suboptimal system utilization are observedin production systems. To address these problems, we propose Netscope, a congestion mitigation framework based on a novel delay sensitivity metric that quantifies the impact of congestionon application runtime. Netscope uses delay sensitivity estimates to drive a congestion mitigation mechanism to selectively throttle applications that are less susceptible to congestion.</description></item><item><title>Measuring Congestion in High-Performance Datacenter Interconnects</title><link>https://apatke.github.io/publications/nsdi2020/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><guid>https://apatke.github.io/publications/nsdi2020/</guid><description>While it is widely acknowledged that network congestion in High Performance Computing (HPC) systems can significantly degrade application performance, there has been little to no quantification of congestion on credit-based interconnect networks. We present a methodology for detecting, extracting, and characterizing regions of congestion in networks. We have implemented the methodology in a deployable tool, Monet, which can provide such analysis and feedback at runtime. Using Monet, we characterize and diagnose congestion in the world’s largest 3D torus network of Blue Waters, a 13.</description></item></channel></rss>
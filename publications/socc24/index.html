<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.59.1"><meta name=author content="Archit Patke"><meta name=description content="Queue Management for SLO-oriented Large Language Model Serving"><meta property="og:title" content="Queue Management for SLO-oriented Large Language Model Serving"><meta property="og:description" content="Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving."><meta property="og:type" content="article"><meta property="og:url" content="https://apatke.github.io/publications/socc24/"><meta property="article:published_time" content="2024-11-20T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-20T00:00:00+00:00"><meta itemprop=name content="Queue Management for SLO-oriented Large Language Model Serving"><meta itemprop=description content="Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving."><meta itemprop=datePublished content="2024-11-20T00:00:00&#43;00:00"><meta itemprop=dateModified content="2024-11-20T00:00:00&#43;00:00"><meta itemprop=wordCount content="206"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Queue Management for SLO-oriented Large Language Model Serving"><meta name=twitter:description content="Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving."><title>Queue Management for SLO-Oriented Large Language Model Serving | Archit Patke</title><link rel=canonical href=https://apatke.github.io/publications/socc24/><link rel=icon href=https://apatke.github.io/img/favicon.ico><link rel="shortcut icon" href=https://apatke.github.io/img/favicon.ico><link rel=apple-touch-icon href=https://apatke.github.io/img/favicon.ico><link rel=stylesheet href=/main.min.css><link rel=preload href=https://use.fontawesome.com/releases/v5.3.1/css/all.css crossorigin=anonymous as=style onload="this.rel='stylesheet'"><link rel=preload href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css type=text/css as=style onload="this.rel='stylesheet'"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-167344484-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=padding id=left></div><div id=container><div id=sidebar><div id=enclose-sidebar><div id=sidebar-content><h1><a href=https://apatke.github.io/>Archit Patke</a></h1><p>PhD Candidate<br>Electrical and Computer Engineering<br>University of Illinois at Urbana-Champaign</p><ul class=icon-list><li><a href=mailto:apatke@illinois.edu><i class="fas fa-envelope-open fa-fw"></i></a></li><li><a href="https://scholar.google.com/citations?user=NK4dg2oAAAAJ&amp;hl=en"><i class="ai ai-google-scholar-square ai-fw"></i></a></li><li><a href=https://www.linkedin.com/in/archit-patke-169a46103/><i class="fab fa-linkedin fa-fw"></i></a></li><li><a href=https://github.com/apatke><i class="fab fa-github fa-fw"></i></a></li></ul></div><div id=avatar><img src=https://apatke.github.io/img/me.jpg alt=Avatar></div></div></div><div id=content><ol class=breadcrumbs><li><a href=/><i class="fas fa-home"></i></a></li><li><a href=https://apatke.github.io/publications/>Publications</a></li><li>Queue Management for SLO-oriented Large Language Model Serving</li></ol><div class=titlesec><h1 class=no-border>Queue Management for SLO-oriented Large Language Model Serving</h1><h3>**Archit Patke**, Dhemath Reddy, Saurabh Jha, Haoran Qiu, Christian Pinto, Chandra Narayanaswami, Zbigniew Kalbarczyk, and Ravishankar Iyer</h3><h3 style=color:#555><b><i>**SoCC 2024**</i></b></h3><i class="fas fa-trophy" aria-hidden=true></i>&nbsp;Integrated into vLLM v0.6.2&#43; [(link)](https://github.com/vllm-project/vllm/pull/5958)</li></div><hr><ul class=single-paper-links><li><a href=https://dl.acm.org/doi/10.1145/3698038.3698523 target=_blank><i class="ai ai-doi" aria-hidden=true></i>&nbsp;DOI</a></li><li><a href=https://github.com/QLM-project/QLM target=_blank><i class="fas fa-code" aria-hidden=true></i>&nbsp;Code</a></li><li><a href=https://apatke.github.io/publications/socc24/Paper.pdf target=_blank><i class="far fa-file-pdf" aria-hidden=true></i>&nbsp;Paper</a></li></ul><h3>Abstract</h3><p>Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving. QLM maintains batch and interactive requests across different models and SLOs in a request queue. Optimal ordering of the request queue is critical to maintain SLOs while ensuring high resource utilization. To generate this optimal ordering, QLM uses a Request Waiting Time (RWT) Estimator that estimates the waiting times for requests in the request queue. These estimates are used by a global scheduler to orchestrate LLM Serving Operations (LSOs) such as request pulling, request eviction, load balancing, and model swapping. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems. QLM&rsquo;s evaluation is based on the production requirements of a cloud provider. QLM is publicly available at <a href=https://www.github.com/QLM-project/QLM>https://www.github.com/QLM-project/QLM</a>.</p></div></div><div class=padding id=right></div><script src=https://instant.page/3.0.0 type=module defer integrity=sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1 crossorigin=anonymous></script></body></html>
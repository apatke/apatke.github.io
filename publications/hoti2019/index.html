<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Hugo 0.59.1"><meta name=author content="Archit Patke"><meta name=description content="A Study of Network Congestion in Two Supercomputing High-Speed Interconnects"><meta property="og:title" content="A Study of Network Congestion in Two Supercomputing High-Speed Interconnects"><meta property="og:description" content="Network congestion in high-speed interconnects is a major source of application runtime performance variation. Recent years have witnessed a surge of interest from both academia and industry in the development of novel approaches for congestion control at the network level and in application placement, mapping, and scheduling at the system-level. However, these studies are based on proxy applications and benchmarks that are not representative of field-congestion characteristics of high-speed interconnects. To address this gap, we present (a) an end-to-end framework for monitoring and analysis to support long-term field-congestion characterization studies, and (b) an empirical study of network congestion in petascale systems across two different interconnect technologies: (i) Cray Gemini, which uses a 3-D torus topology, and (ii) Cray Aries, which uses the DragonFly topology."><meta property="og:type" content="article"><meta property="og:url" content="https://apatke.github.io/publications/hoti2019/"><meta property="article:published_time" content="2019-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2019-02-26T00:00:00+00:00"><meta itemprop=name content="A Study of Network Congestion in Two Supercomputing High-Speed Interconnects"><meta itemprop=description content="Network congestion in high-speed interconnects is a major source of application runtime performance variation. Recent years have witnessed a surge of interest from both academia and industry in the development of novel approaches for congestion control at the network level and in application placement, mapping, and scheduling at the system-level. However, these studies are based on proxy applications and benchmarks that are not representative of field-congestion characteristics of high-speed interconnects. To address this gap, we present (a) an end-to-end framework for monitoring and analysis to support long-term field-congestion characterization studies, and (b) an empirical study of network congestion in petascale systems across two different interconnect technologies: (i) Cray Gemini, which uses a 3-D torus topology, and (ii) Cray Aries, which uses the DragonFly topology."><meta itemprop=datePublished content="2019-02-26T00:00:00&#43;00:00"><meta itemprop=dateModified content="2019-02-26T00:00:00&#43;00:00"><meta itemprop=wordCount content="124"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="A Study of Network Congestion in Two Supercomputing High-Speed Interconnects"><meta name=twitter:description content="Network congestion in high-speed interconnects is a major source of application runtime performance variation. Recent years have witnessed a surge of interest from both academia and industry in the development of novel approaches for congestion control at the network level and in application placement, mapping, and scheduling at the system-level. However, these studies are based on proxy applications and benchmarks that are not representative of field-congestion characteristics of high-speed interconnects. To address this gap, we present (a) an end-to-end framework for monitoring and analysis to support long-term field-congestion characterization studies, and (b) an empirical study of network congestion in petascale systems across two different interconnect technologies: (i) Cray Gemini, which uses a 3-D torus topology, and (ii) Cray Aries, which uses the DragonFly topology."><title>A Study of Network Congestion in Two Supercomputing High-Speed Interconnects | Archit Patke</title><link rel=canonical href=https://apatke.github.io/publications/hoti2019/><link rel=icon href=https://apatke.github.io/img/favicon.ico><link rel="shortcut icon" href=https://apatke.github.io/img/favicon.ico><link rel=apple-touch-icon href=https://apatke.github.io/img/favicon.ico><link rel=stylesheet href=/main.min.css><link rel=preload href=https://use.fontawesome.com/releases/v5.3.1/css/all.css crossorigin=anonymous as=style onload="this.rel='stylesheet'"><link rel=preload href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css type=text/css as=style onload="this.rel='stylesheet'"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-167344484-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><div class=padding id=left></div><div id=container><div id=sidebar><div id=enclose-sidebar><div id=sidebar-content><h1><a href=https://apatke.github.io/>Archit Patke</a></h1><p>PhD Student<br>Electrical and Computer Engineering<br>University of Illinois at Urbana-Champaign</p><ul class=icon-list><li><a href=mailto:apatke@illinois.edu><i class="fas fa-envelope-open fa-fw"></i></a></li><li><a href="https://illinois.edu/map/view?buildingId=148"><i class="fas fa-map-marker-alt fa-fw"></i></a></li><li><a href=https://dblp.uni-trier.de/pers/hd/p/Patke:Archit><i class="ai ai-dblp fa-fw"></i></a></li><li><a href=https://www.linkedin.com/in/archit-patke-169a46103/><i class="fab fa-linkedin fa-fw"></i></a></li><li><a href=https://github.com/apatke><i class="fab fa-github fa-fw"></i></a></li><li><a href=https://www.facebook.com/archit.patke><i class="fab fa-facebook fa-fw"></i></a></li></ul></div><div id=avatar><img src=https://apatke.github.io/img/me.jpg alt=Avatar></div></div></div><div id=content><ol class=breadcrumbs><li><a href=/><i class="fas fa-home"></i></a></li><li><a href=https://apatke.github.io/publications/>Publications</a></li><li>A Study of Network Congestion in Two Supercomputing High-Speed Interconnects</li></ol><div class=titlesec><h1 class=no-border>A Study of Network Congestion in Two Supercomputing High-Speed Interconnects</h1><h3>Saurabh Jha, **Archit Patke**, Jim Brandt, Ann Gentile, Mike Showerman, Eric Roman, Zbigniew Kalbarczyk, William Kramer and Ravishankar Iyer</h3><h3 style=color:#555><b><i>IEEE Hot Interconnects 2019</i></b></h3></div><hr><ul class=single-paper-links><li><a href=https://ieeexplore.ieee.org/document/9070292 target=_blank><i class="ai ai-doi" aria-hidden=true></i>&nbsp;DOI</a></li><li><a href=https://apatke.github.io/publications/hoti2019/Paper.pdf target=_blank><i class="far fa-file-pdf" aria-hidden=true></i>&nbsp;Paper</a></li></ul><h3>Abstract</h3><p>Network congestion in high-speed interconnects is
a major source of application runtime performance variation.
Recent years have witnessed a surge of interest from both
academia and industry in the development of novel approaches
for congestion control at the network level and in application
placement, mapping, and scheduling at the system-level. However,
these studies are based on proxy applications and benchmarks
that are not representative of field-congestion characteristics of
high-speed interconnects. To address this gap, we present (a) an
end-to-end framework for monitoring and analysis to support
long-term field-congestion characterization studies, and (b) an
empirical study of network congestion in petascale systems across
two different interconnect technologies: (i) Cray Gemini, which
uses a 3-D torus topology, and (ii) Cray Aries, which uses the
DragonFly topology.</p></div></div><div class=padding id=right></div><script src=https://instant.page/3.0.0 type=module defer integrity=sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1 crossorigin=anonymous></script></body></html>